<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Leaderboard · InventoryBench</title>
  <meta name="description" content="InventoryBench: a 1,320-instance benchmark for OR and LLM-based inventory agents on synthetic and real demand trajectories.">
  <link rel="stylesheet" href="assets/css/main.css">
</head>
<body>
  <header class="site-header">
    <div class="wrapper header-inner">
      <div class="site-branding">
        <a class="site-title" href="index.html">InventoryBench</a>
        <p class="site-tagline">Benchmark for OR- and LLM-based inventory agents</p>
      </div>
      <nav class="site-nav">
        <a href="index.html">Home</a>
        <a href="dataset.html">Dataset</a>
        <a href="leaderboard.html">Leaderboard</a>
      </nav>
    </div>
  </header>

  <main class="page-content">
    <div class="wrapper">
      <article class="page">
        <div class="page-content">
          <h1>Leaderboard</h1>
          <p>This page summarizes performance of OR baselines, LLM policies, and OR–LLM hybrids on <strong>InventoryBench</strong>.</p>
          <p>The primary evaluation metric is <strong>cumulative reward</strong> over the test period. We also report <strong>normalized reward</strong>, defined as the ratio between actual reward and a perfect‑foresight upper bound.</p>

          <h2>Overall leaderboard</h2>
          <p>The table below ranks all evaluated combinations of LLM and decision method by <strong>average normalized reward</strong> across all 1,320 benchmark instances. Higher is better, and 1.0 corresponds to the perfect‑foresight upper bound.</p>

          <div id="leaderboard-container">
            <div class="table-wrapper">
              <table class="leaderboard-table">
                <thead>
                  <tr>
                    <th>Rank</th>
                    <th>Agent</th>
                    <th>Avg Normalized Reward</th>
                    <th>Details</th>
                  </tr>
                </thead>
                <tbody id="leaderboard-tbody">
                  <tr>
                    <td colspan="5" style="text-align: center; padding: 2rem;">Loading...</td>
                  </tr>
                </tbody>
              </table>
            </div>
          </div>

          <h2>Metric definition</h2>
          <p>For each instance, cumulative reward is:</p>
          <pre><code>Reward = Σ_t (profit_t × units_sold_t − holding_cost_t × units_held_t)</code></pre>
          <p>where:</p>
          <ul>
            <li><code>units_sold_t = min(demand_t, available_inventory_t)</code></li>
            <li><code>units_held_t</code> is the end‑of‑period on‑hand inventory.</li>
          </ul>
          <p>The <strong>perfect score</strong> assumes perfect knowledge of future demand and unlimited supply:</p>
          <pre><code>PerfectScore = Σ_t (demand_t × profit_t)</code></pre>
          <p>We report <strong>normalized reward</strong> as:</p>
          <pre><code>NormalizedReward = Reward / PerfectScore</code></pre>
          <p>This normalizes performance across instances with different demand scales and cost parameters.</p>

          <h2>Method-level breakdown (per LLM × method)</h2>
          <p>For each row in the leaderboard, the detail page shows:</p>
          <ul>
            <li>The underlying model name (for example, <code>google/gemini-3-flash-preview</code>).</li>
            <li>Average normalized reward by dataset family:
              <ul>
                <li><code>synthetic_trajectory</code> (720 instances)</li>
                <li><code>real_trajectory</code> (600 instances)</li>
              </ul>
            </li>
            <li>Average normalized reward by lead‑time setting:
              <ul>
                <li><code>lead_time_0</code>, <code>lead_time_4</code>, <code>lead_time_stochastic</code></li>
              </ul>
            </li>
          </ul>
          <p>In future iterations we can further extend these pages with per‑instance tables and links into raw <code>benchmark_results.json</code> files.</p>
        </div>
      </article>
    </div>
  </main>

  <footer class="site-footer">
    <div class="wrapper footer-inner">
      <p class="footer-title">InventoryBench</p>
      <p class="footer-description">A 1,320-instance benchmark for OR and LLM-based inventory agents on synthetic and real demand trajectories.</p>
      <p class="footer-links">
        <a href="https://github.com/BrunoFu/OR_Agent" target="_blank" rel="noopener">Code &amp; Data on GitHub</a>
      </p>
    </div>
  </footer>

  <script type="application/json" id="leaderboard-data">{"methods":[{"llm_id":"gemini-3-flash","llm_label":"Gemini 3 Flash","method_id":"or_to_llm","method_label":"OR\u2192LLM","mean_ratio":0.5380},{"llm_id":"grok-4.1-fast","llm_label":"Grok 4.1 Fast","method_id":"or_to_llm","method_label":"OR\u2192LLM","mean_ratio":0.5139},{"llm_id":"gemini-3-flash","llm_label":"Gemini 3 Flash","method_id":"llm_to_or","method_label":"LLM\u2192OR","mean_ratio":0.5009},{"llm_id":"gemini-3-flash","llm_label":"Gemini 3 Flash","method_id":"llm","method_label":"LLM","mean_ratio":0.4945},{"llm_id":"grok-4.1-fast","llm_label":"Grok 4.1 Fast","method_id":"llm_to_or","method_label":"LLM\u2192OR","mean_ratio":0.4934},{"llm_id":"gpt-5-mini","llm_label":"GPT-5 Mini","method_id":"or_to_llm","method_label":"OR\u2192LLM","mean_ratio":0.4611},{"llm_id":"gpt-5-mini","llm_label":"GPT-5 Mini","method_id":"llm","method_label":"LLM","mean_ratio":0.4597},{"llm_id":"grok-4.1-fast","llm_label":"Grok 4.1 Fast","method_id":"llm","method_label":"LLM","mean_ratio":0.4593},{"llm_id":"gpt-5-mini","llm_label":"GPT-5 Mini","method_id":"llm_to_or","method_label":"LLM\u2192OR","mean_ratio":0.4578},{"llm_id":"or-baseline","llm_label":"","method_id":"or","method_label":"OR","mean_ratio":0.4447}]}</script>
  <script>
    (function () {
      var data = JSON.parse(document.getElementById('leaderboard-data').textContent);
      var methods = data.methods || [];
      methods.sort(function (a, b) { return b.mean_ratio - a.mean_ratio; });
      var tbody = document.getElementById('leaderboard-tbody');
      tbody.innerHTML = '';
      methods.forEach(function (row, i) {
        var tr = document.createElement('tr');
        var agentName;
        if (row.method_id === 'or') {
          agentName = 'OR (Base-stock)';
        } else {
          agentName = row.llm_label + ' (' + row.method_label + ')';
        }
        var detailLink = row.method_id === 'or'
          ? '<a href="leaderboard/grok-4.1-fast/or.html">View</a>'
          : '<a href="leaderboard/' + row.llm_id + '/' + row.method_id + '.html">View</a>';
        tr.innerHTML = '<td>' + (i + 1) + '</td><td>' + agentName + '</td><td>' + row.mean_ratio.toFixed(4) + '</td><td>' + detailLink + '</td>';
        tbody.appendChild(tr);
      });
    })();
  </script>
</body>
</html>
